# -*- coding: utf-8 -*-
"""single digits

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SMQ09_2T61c0GB7eA39ml_s1YCPmYx1Z
"""

#Import libraries
import sys
print(sys.argv[0])
import math
import tensorflow as tf
import os
import matplotlib.pyplot as plt
import pandas as pd
import h5py
import numpy as np


np.random.seed(123)
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from keras.layers import LSTM
from keras.models import Sequential
from keras.utils import np_utils
from keras.models import Sequential
from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
import functools

#Functions to read data from .hdf5 files

def load_dict_from_hdf5(filename):
    with h5py.File(filename, 'r') as h5file:
        return recursively_load_dict_contents_from_group(h5file, '/')
    h5file.close()

def recursively_load_dict_contents_from_group(h5file, path):
    ans = {}
    for key, item in h5file[path].items():
        if isinstance(item, h5py._hl.dataset.Dataset):
            ans[key] = item.value
        elif isinstance(item, h5py._hl.group.Group):
            ans[key] = recursively_load_dict_contents_from_group(h5file, path + key + '/')
    return ans
def convert_tensor(arg):
    return tf.convert_to_tensor(arg, dtype=tf.float32)

#Import data from Google Drive

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)
filename = "/content/drive/My Drive/ColabPro/train (2).hdf5"
filename2 = "/content/drive/My Drive/ColabPro/validation.hdf5"
data = load_dict_from_hdf5(filename)
val_data = load_dict_from_hdf5(filename2)

#Filter data from input

label_encoder = LabelEncoder()
onehot_encoder = OneHotEncoder(sparse=False)
unique_labels = {}

def to_lists(data):
    labels_lst = []
    curves_lst = []
    max_curves = 0
    for a in data:
        for b in data[a]:
            label = data[a][b]['label']
            if(label not in unique_labels): unique_labels[label] = 1
            labels_lst.append(label)
            sample = np.concatenate(list(data[a][b]['feat_bez_curves'].values()))
            sample = np.nan_to_num(sample)
            curves_lst.append(sample)
            max_curves = max(max_curves,sample.shape[0])
    return curves_lst, labels_lst, max_curves
        
#From here: https://stackoverflow.com/questions/57346556/creating-a-ragged-tensor-from-a-list-of-tensors
def stack_ragged(tensors):
    values = tf.concat(tensors, axis=0)
    lens = tf.stack([tf.shape(t, out_type=tf.int64)[0] for t in tensors])
    return tf.RaggedTensor.from_row_lengths(values, lens)

def stack_dense(tensors,max_curves):
    pad = lambda x:np.pad(x, pad_width=((0,max_curves-len(x)),(0,0)), constant_values = (-99))
    return np.concatenate([np.expand_dims(pad(x),0) for x in tensors],axis=0)
def encode_labels(labels,fit=True):
    
    if(fit): label_encoder.fit(labels)
    #print(label_encoder.classes_)
    #rint(label_encoder.classes_.shape)
    integer_encoded = label_encoder.transform(labels)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    if(fit): onehot_encoder.fit(integer_encoded)
    onehot_encoded = onehot_encoder.transform(integer_encoded)
    return onehot_encoded

#Process train_data
curves_lst, labels_lst, max_curves = to_lists(data)
X_train = stack_dense(curves_lst,max_curves)
Y_train = encode_labels(np.array(labels_lst))

#This is just a for a sanity check
x_unq = unique_labels
A = label_encoder.transform(sorted(list(x_unq.keys())))
unique_labels = {}

#Process val_data
curves_lst, labels_lst, max_curves = to_lists(val_data)
#X_val = stack_ragged(curves_lst)
X_val = stack_dense(curves_lst,max_curves)
Y_val = encode_labels(np.array(labels_lst),fit=False)
#This is just a for a sanity check
B = label_encoder.transform(sorted(list(unique_labels.keys())))

print(X_train.shape)
print(Y_train.shape)
print(X_val.shape)
print(Y_val.shape)

#Model
seq = [
    tf.keras.Input(shape=(None,9)),
    tf.keras.layers.Masking(mask_value = -99),
    tf.keras.layers.LSTM(300, dropout=0.3, recurrent_dropout=0.3),
    tf.keras.layers.Dense(Y_train.shape[1], activation = "softmax")
]

#Separate model into with-mask and without-mask since masking layer is not working with Tensorflow.JS for now
seq_no_mask = seq[:1] + seq[2:]
keras_model = tf.keras.Sequential(seq)

#Top5 & Top3 Accuracy to check
top5_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=5)
top5_acc.__name__ = 'top5_acc'
top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)
top3_acc.__name__ = 'top3_acc'

opt = tf.keras.optimizers.Adam(
    learning_rate= 0.001 * 10)
keras_model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy', top3_acc, top5_acc])

history = keras_model.fit(X_train, Y_train, epochs = 25, batch_size = 3200, validation_split = 0.15)
print(keras_model.summary())

#Plot out the training process for analysis purpose

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model train vs validation loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'])

#Evaluate the model with testing data

print('\n# Evaluate on 2013 test data')
results = keras_model.evaluate(X_val, Y_val)
print('test loss, test acc:', results)

#Put trained weights back in the without-mask model that gets deployed to server
final_model = tf.keras.Sequential(seq_no_mask)
final_model.set_weights(keras_model.get_weights())

import tensorflowjs

#Export model into .h5 and .json file to deploy on React.JS server
tensorflowjs.converters.save_keras_model(final_model, '/finally_model1_js/')

final_model.get_weights()

