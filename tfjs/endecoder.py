# -*- coding: utf-8 -*-
"""endecoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UwVljyFWpAaotIfnol8YJl9KbNirltU6
"""

import sys
print(sys.argv[0])
import math
import tensorflow as tf
import os
import matplotlib.pyplot as plt
import pandas as pd
import h5py
import numpy as np


np.random.seed(123)
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Embedding, Dropout
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers.schedules import LearningRateSchedule
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy

from keras.layers import LSTM
from keras.models import Sequential
from keras.utils import np_utils
from keras.models import Sequential
from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
import functools
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from sklearn.model_selection import train_test_split

import unicodedata
import re
import numpy as np
import os
import io
import time
from time import sleep
from tqdm import tqdm

def load_dict_from_hdf5(filename):
    with h5py.File(filename, 'r') as h5file:
        return recursively_load_dict_contents_from_group(h5file, '/')
    h5file.close()

def recursively_load_dict_contents_from_group(h5file, path):
    ans = {}
    for key, item in h5file[path].items():
        if isinstance(item, h5py._hl.dataset.Dataset):
            ans[key] = item.value
        elif isinstance(item, h5py._hl.group.Group):
            ans[key] = recursively_load_dict_contents_from_group(h5file, path + key + '/')
    return ans
def convert_tensor(arg):
    return tf.convert_to_tensor(arg, dtype=tf.float32)

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)
filename = "/content/drive/My Drive/ColabPro/train.hdf5"
filename2 = "/content/drive/My Drive/ColabPro/validation.hdf5"
data = load_dict_from_hdf5(filename)
val_data = load_dict_from_hdf5(filename2)

label_encoder = LabelEncoder()
onehot_encoder = OneHotEncoder(sparse=False)
unique_labels = {}

def to_lists(data):
    labels_lst = []
    curves_lst = []
    max_curves = 0
    for a in data:
        for b in data[a]:
            label = data[a][b]['label']
            if(label not in unique_labels): unique_labels[label] = 1
            labels_lst.append([label])
            sample = np.concatenate(list(data[a][b]['feat_bez_curves'].values()))
            sample = np.nan_to_num(sample)
            curves_lst.append(sample)
            max_curves = max(max_curves,sample.shape[0])
    return curves_lst, labels_lst, max_curves
        
#From here: https://stackoverflow.com/questions/57346556/creating-a-ragged-tensor-from-a-list-of-tensors
def stack_ragged(tensors):
    values = tf.concat(tensors, axis=0)
    lens = tf.stack([tf.shape(t, out_type=tf.int64)[0] for t in tensors])
    return tf.RaggedTensor.from_row_lengths(values, lens)

def stack_dense(tensors,max_curves):
    pad = lambda x:np.pad(x, pad_width=((0,max_curves-len(x)),(0,0)), constant_values = (-99))
    return np.concatenate([np.expand_dims(pad(x),0) for x in tensors],axis=0)
def encode_labels(labels,fit=True):
    
    if(fit): label_encoder.fit(labels)
    #print(label_encoder.classes_)
    #rint(label_encoder.classes_.shape)
    integer_encoded = label_encoder.transform(labels)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    if(fit): onehot_encoder.fit(integer_encoded)
    onehot_encoded = onehot_encoder.transform(integer_encoded)
    return onehot_encoded

#Process train_data
curves_lst, labels_lst, max_curves = to_lists(data)
#X_train = stack_ragged(curves_lst)
X_train = stack_dense(curves_lst,max_curves)
Y_train = encode_labels(np.array(labels_lst))
#This is just a for a sanity check
#x_unq = unique_labels
#A = label_encoder.transform(sorted(list(x_unq.keys())))
unique_labels = {}

#Process val_data
curves_lst, labels_lst, max_curves = to_lists(val_data)
#X_val = stack_ragged(curves_lst)
X_val = stack_dense(curves_lst,max_curves)
Y_val = encode_labels(np.array(labels_lst))
#This is just a for a sanity check
#B = label_encoder.transform(sorted(list(unique_labels.keys())))

print(X_train.shape)
print(Y_train.shape)
print(X_val.shape)
print(X_train[0])

class MaskHandler(object):
    def padding_mask(self, sequence):
        sequence = tf.cast(tf.math.equal(sequence, -99), tf.float32)
        return sequence[:, tf.newaxis, tf.newaxis, :]

    def look_ahead_mask(self, size):
        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
        return mask

maskHandler = MaskHandler()

x = X_train[0]
print(x.shape)
mask = maskHandler.padding_mask(x)
print("Padding Mask Example:")
print("-----------")
print(mask)
print("-----------")

class ScaledDotProductAttentionLayer():
    def calculate_output_weights(self, q, k, v, mask):
        qk = tf.matmul(q, k, transpose_b=True)
        dk = tf.cast(tf.shape(k)[-1], tf.float32)
        scaled_attention = qk / tf.math.sqrt(dk)

        if mask is not None:
            scaled_attention_logits += (mask * -1e9)  

        weights = tf.nn.softmax(scaled_attention, axis=-1)
        output = tf.matmul(weights, v)

        return output, weights

class MultiHeadAttentionLayer(Layer):
    def __init__(self, num_neurons, num_heads):
        super(MultiHeadAttentionLayer, self).__init__()
        
        self.num_heads = num_heads
        self.num_neurons = num_neurons
        self.depth = num_neurons // self.num_heads
        self.attention_layer = ScaledDotProductAttentionLayer()
        
        self.q_layer = Dense(num_neurons)
        self.k_layer = Dense(num_neurons)
        self.v_layer = Dense(num_neurons)

        self.linear_layer = Dense(num_neurons)

    def split(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q, mask):
        batch_size = tf.shape(q)[0]

        # Run through linear layers
        q = self.q_layer(q)
        k = self.k_layer(k)
        v = self.v_layer(v)

        # Split the heads
        q = self.split(q, batch_size)
        k = self.split(k, batch_size)
        v = self.split(v, batch_size)

        # Run through attention
        attention_output, weights = self.attention_layer.calculate_output_weights(q, k, v, mask)
        
        # Prepare for the rest of processing
        output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(output, (batch_size, -1, self.num_neurons))
        
        # Run through final linear layer
        output = self.linear_layer(concat_attention)

        return output, weights

class PreProcessingLayer(Layer):
    def __init__(self, num_neurons, vocabular_size):
        super(PreProcessingLayer, self).__init__()
        
        # Initialize
        self.num_neurons = num_neurons

        # Add embedings and positional encoding
        self.embedding = Embedding(vocabular_size, self.num_neurons)
        positional_encoding_handler = PositionalEncoding(vocabular_size, self.num_neurons)
        self.positional_encoding = positional_encoding.get_positional_encoding()

        # Add embedings and positional encoding
        self.dropout = Dropout(0.1)
    
    def call(self, sequence, training, mask):
        sequence_lenght = tf.shape(sequence)[1]
        sequence = self.embedding(sequence)

        sequence *= tf.math.sqrt(tf.cast(self.num_neurons, tf.float32))
        sequence += self.positional_encoding[:, :sequence_lenght, :]
        sequence = self.dropout(sequence, training=training)
        
        return sequence

def build_multi_head_attention_layers(num_neurons, num_heads):
    multi_head_attention_layer = MultiHeadAttentionLayer(num_neurons, num_heads)   
    dropout = tf.keras.layers.Dropout(0.1)
    normalization = LayerNormalization(epsilon=1e-6)
    return multi_head_attention_layer, dropout, normalization

def build_feed_forward_layers(num_neurons, num_hidden_neurons):
    feed_forward_layer = tf.keras.Sequential()
    feed_forward_layer.add(Dense(num_hidden_neurons, activation='relu'))
    feed_forward_layer.add(Dense(num_neurons))
        
    dropout = Dropout(0.1)
    normalization = LayerNormalization(epsilon=1e-6)
    return feed_forward_layer, dropout, normalization

class EncoderLayer(Layer):
    def __init__(self, num_neurons, num_hidden_neurons, num_heads):
        super(EncoderLayer, self).__init__()

        # Build multi head attention layer and necessary additional layers
        self.multi_head_attention_layer, self.attention_dropout, self.attention_normalization = \
        build_multi_head_attention_layers(num_neurons, num_heads)   
            
        # Build feed-forward neural network and necessary additional layers
        self.feed_forward_layer, self.feed_forward_dropout, self.feed_forward_normalization = \
        build_feed_forward_layers(num_neurons, num_hidden_neurons)
       
    def call(self, sequence, training, mask):

        # Calculate attention output
        attnention_output, _ = self.multi_head_attention_layer(sequence, sequence, sequence, mask)
        attnention_output = self.attention_dropout(attnention_output, training=training)
        attnention_output = self.attention_normalization(sequence + attnention_output)
        
        # Calculate output of feed forward network
        output = self.feed_forward_layer(attnention_output)
        output = self.feed_forward_dropout(output, training=training)
        
        # Combine two outputs
        output = self.feed_forward_normalization(attnention_output + output)

        return output

class DecoderLayer(Layer):
    def __init__(self, num_neurons, num_hidden_neurons, num_heads):
        super(DecoderLayer, self).__init__()

        # Build multi head attention layers and necessary additional layers
        self.multi_head_attention_layer1, self.attention_dropout1, self.attention_normalization1 =\
        build_multi_head_attention_layers(num_neurons, num_heads)   
        
        self.multi_head_attention_layer2, self.attention_dropout2, self.attention_normalization2 =\
        build_multi_head_attention_layers(num_neurons, num_heads)           

        # Build feed-forward neural network and necessary additional layers
        self.feed_forward_layer, self.feed_forward_dropout, self.feed_forward_normalization = \
        build_feed_forward_layers(num_neurons, num_hidden_neurons)

    def call(self, sequence, enconder_output, training, look_ahead_mask, padding_mask):

        attnention_output1, attnention_weights1 = self.multi_head_attention_layer1(sequence, sequence, sequence, look_ahead_mask)
        attnention_output1 = self.attention_dropout1(attnention_output1, training=training)
        attnention_output1 = self.attention_normalization1(sequence + attnention_output1)
        
        attnention_output2, attnention_weights2 = self.multi_head_attention_layer2(enconder_output, enconder_output, attnention_output1, padding_mask)
        attnention_output2 = self.attention_dropout1(attnention_output2, training=training)
        attnention_output2 = self.attention_normalization1(attnention_output1 + attnention_output2)

        output = self.feed_forward_layer(attnention_output2)
        output = self.feed_forward_dropout(output, training=training)
        output = self.feed_forward_normalization(attnention_output2 + output)

        return output, attnention_weights1, attnention_weights2

class Encoder(Layer):
    def __init__(self, num_neurons, num_hidden_neurons, num_heads, vocabular_size, num_enc_layers = 6):
        super(Encoder, self).__init__()
        
        self.num_enc_layers = num_enc_layers
        
        self.pre_processing_layer = PreProcessingLayer(num_neurons, vocabular_size)
        self.encoder_layers = [EncoderLayer(num_neurons, num_hidden_neurons, num_heads) for _ in range(num_enc_layers)]

    def call(self, sequence, training, mask):
        
        sequence = self.pre_processing_layer(sequence, training, mask)
        for i in range(self.num_enc_layers):
            sequence = self.encoder_layers[i](sequence, training, mask)

        return sequence

class Decoder(Layer):
    def __init__(self, num_neurons, num_hidden_neurons, num_heads, vocabular_size, num_dec_layers=6):
        super(Decoder, self).__init__()

        self.num_dec_layers = num_dec_layers
        
        self.pre_processing_layer = PreProcessingLayer(num_neurons, vocabular_size)
        self.decoder_layers = [DecoderLayer(num_neurons, num_hidden_neurons, num_heads) for _ in range(num_dec_layers)]

    def call(self, sequence, enconder_output, training, look_ahead_mask, padding_mask):
            
        sequence = self.pre_processing_layer(sequence, training, mask)
        
        for i in range(self.num_dec_layers):

            sequence, attention_weights1, attention_weights2 = self.dec_layers[i](sequence, enconder_output, training, look_ahead_mask, padding_mask)

            attention_weights['decoder_layer{}_attention_weights1'.format(i+1)] = attention_weights1
            attention_weights['decoder_layer{}_attention_weights2'.format(i+1)] = attention_weights2

        return sequence, attention_weights

class Transformer(Model):
    def __init__(self, num_layers, num_neurons, num_hidden_neurons, num_heads, input_vocabular_size, target_vocabular_size):
        super(Transformer, self).__init__()
        self.encoder = Encoder(num_neurons, num_hidden_neurons, num_heads, input_vocabular_size, num_layers)
        self.decoder = Decoder(num_neurons, num_hidden_neurons, num_heads, target_vocabular_size, num_layers)
        self.linear_layer = Dense(target_vocabular_size)

    def call(self, transformer_input, tar, training, encoder_padding_mask, look_ahead_mask, decoder_padding_mask):
        encoder_output = self.encoder(transformer_input, training, encoder_padding_mask)
        decoder_output, attention_weights = self.decoder(tar, encoder_output, training, look_ahead_mask, decoder_padding_mask)
        output = self.linear_layer(decoder_output)

        return output, attention_weights

class Schedule(LearningRateSchedule):
    def __init__(self, num_neurons, warmup_steps=4000):
        super(Schedule, self).__init__()

        self.num_neurons = tf.cast(num_neurons, tf.float32)
        self.warmup_steps = warmup_steps

    def __call__(self, step):
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)

        return tf.math.rsqrt(self.num_neurons) * tf.math.minimum(arg1, arg2)

loss_objective_function = SparseCategoricalCrossentropy(from_logits=True, reduction='none')

def padded_loss_function(real, prediction):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss = loss_objective_function(real, prediction)

    mask = tf.cast(mask, dtype=loss.dtype)
    loss *= mask

    return tf.reduce_mean(loss)

training_loss = Mean(name='training_loss')
training_accuracy = SparseCategoricalAccuracy(name='training_accuracy')

# Initialize helpers
maskHandler = MaskHandler()

# Initialize parameters
num_layers = 4
num_neurons = 128
num_hidden_layers = 512
num_heads = 8

# Initialize vocabular size
input_vocablar_size = X_train.shape[0]
target_vocablar_size = Y_train.shape[0]

# Initialize learning rate
learning_rate = Schedule(num_neurons)
optimizer = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

# Initialize transformer
transformer = Transformer(num_layers, num_neurons, num_hidden_layers, num_heads, input_vocablar_size, target_vocablar_size)

BUFFER_SIZE = len(X_train)
BATCH_SIZE = 128
UNIT = 200
steps_per_epoch = len(X_train)//BATCH_SIZE
print(steps_per_epoch)
embedding_dim_X = X_train.shape[1]
embedding_dim_Y = Y_train.shape[1]
dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)





"""____________________________________________________________________"""

example_input_batch, example_target_batch = next(iter(dataset))
example_input_batch.shape, example_target_batch.shape
#print(example_target_batch)
#print(example_input_batch[0])
#print(example_input_batch[0])

class Encoder(tf.keras.Model):
  def __init__(self, batch_size, dim, units):
    super(Encoder, self).__init__()
    self.dim = dim
    self.batch_sz = batch_size
    self.units = units
    self.gru = tf.keras.layers.GRU(self.units,
                                return_sequences=True,
                                return_state=True)
  def call(self, inputs, hidden):
    output, state = self.gru(inputs, initial_state = hidden)
    return output, state
  
  def initialize_hidden_state(self):
    return tf.zeros((self.batch_sz, self.units))

encoder = Encoder(example_input_batch.shape[0], embedding_dim_X, UNIT)
sample_enc_output, sample_enc_hidden = encoder(example_input_batch, encoder.initialize_hidden_state())
print(example_input_batch.shape, encoder.initialize_hidden_state().shape)
print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_enc_output.shape))
print ('Encoder state shape: (batch size, units) {}'.format(sample_enc_hidden.shape))

class Attention(tf.keras.layers.Layer):
  def __init__(self, dim, units):
    super(Attention, self).__init__();
    self.W = tf.keras.layers.Dense(dim)
    self.U = tf.keras.layers.Dense(dim)
    self.V = tf.keras.layers.Dense(1)
    self.units = units
  def call(self, hidden_state , annotation): 

    hidden_state = tf.expand_dims(hidden_state, axis = 1)

    #Bahdanau attention
    #score = vT * tanh(Watt * hidden_state + Uatt * annotation)
    score = self.V(tf.nn.tanh(
        self.W(hidden_state) + self.U(annotation)))

    #attention weight = softmax of score 
    attention_weights = tf.nn.softmax(score, axis = 1)

    #Ct = att_weights * annotation vector
    context_vector = attention_weights * annotation
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

attention_layer = Attention(10, UNIT)
attention_result, attention_weights = attention_layer(sample_enc_hidden, sample_enc_output)

print("Attention result shape: (batch size, units) {}".format(attention_result.shape))
print("Attention weights shape: (batch_size, sequence_length, 1) {}".format(attention_weights.shape))

class Decoder(tf.keras.Model):
  def __init__(self, batch_sz, embedding_dim, units):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.embedding_dim = embedding_dim
    self.units = units;
    self.embedding = tf.keras.layers.Embedding(self.embedding_dim, self.units)
    self.gru = tf.keras.layers.GRU(self.units,
                                   return_sequences=True,
                                   return_state=True)
    self.fc = tf.keras.layers.Dense(self.embedding_dim)

    self.attention = Attention(self.batch_sz, UNIT)

  def call(self, x, hidden, enc_output):
    context_vector, attention_weights = self.attention(hidden, enc_output)
    x = self.embedding(x)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
    output, state = self.gru(x)

    output = tf.reshape(output, (-1, output.shape[2]))

    x = self.fc(output)

    return x, state, attention_weights

decoder = Decoder(BATCH_SIZE, embedding_dim_Y, UNIT)
z = tf.random.uniform((BATCH_SIZE, 1))
print(z.shape)
sample_dec_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_enc_hidden, sample_enc_output)
print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_dec_output.shape))

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, -99))
  loss_ = loss_object(real, pred)

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tf.reduce_mean(loss_)

checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)

@tf.function
def train_step(inp, targ, enc_hidden):
  loss = 0
  print('Start training')
  with tf.GradientTape() as tape:
    #print('here1')
    enc_output, enc_hidden = encoder(inp, enc_hidden)
    
    dec_hidden = enc_hidden
    #print('here2')
    dec_input = tf.random.uniform((BATCH_SIZE, 1))
    #print(dec_input.shape)
    
    # Teacher forcing - feeding the target as the next input
    print('Teacher forcing ...')
    for t in range(1, targ.shape[1]):
        # passing enc_output to the decoder
        #print(t)
        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)
        #print('Output shape: ', predictions.shape)
        loss += loss_function(targ[:, t], predictions)

        # using teacher forcing
        dec_input = tf.expand_dims(targ[:, t], 1)

        #Track progress

  batch_loss = (loss / int(targ.shape[1]))

  variables = encoder.trainable_variables + decoder.trainable_variables

  print('Calculating gradients ..')
  gradients = tape.gradient(loss, variables)
  print('Applying gradients ..')
  optimizer.apply_gradients(zip(gradients, variables))
  #print('Batch loss: {:.4f} ', batch_loss.numpy())
  return batch_loss

import tensorflow as tf
tf.test.gpu_device_name()
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices()[-1])

EPOCHS = 1

for epoch in range(EPOCHS):
  start = time.time()

  enc_hidden = encoder.initialize_hidden_state()
  total_loss = 0

  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
    batch_loss = train_step(inp, targ, enc_hidden)
    print()
    total_loss += batch_loss
    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,
                                                   batch,
                                                   batch_loss.numpy()))
  # saving (checkpoint) the model every 2 epochs
  checkpoint.save(file_prefix = checkpoint_prefix)

  print('Epoch {} Loss {:.4f}'.format(epoch + 1,
                                      total_loss / steps_per_epoch))
  print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

# restoring the latest checkpoint in checkpoint_dir
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

dict = ['!' ,'(' ,')', '+', ',', '-', '.', '/', '0', '1', '2', '3' ,'4', '5', '6', '7', '8', '9',
'=' ,'A', 'B', 'C', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N', 'P', 'R', 'S', 'T' ,'V', 'X',
'Y', '[', '\\Delta' ,'\\alpha', '\\beta', '\\cos', '\\div', '\\exists',
'\\forall', '\\gamma', '\\geq', '\\gt' ,'\\in', '\\infty', '\\int', '\\lambda',
'\\ldots', '\\leq' ,'\\lim', '\\log' ,'\\lt', '\\mu', '\\neq', '\\phi', '\\pi',
'\\pm', '\\prime' ,'\\rightarrow' ,'\\sigma', '\\sin', '\\sqrt', '\\sum',
'\\tan', '\\theta' ,'\\times', '\\{', '\\}', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g',
'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',
'z', '|']

translate(X_train[0])

def evaluate(inputs):
  #print(inputs.shape)
  attention_plot = np.zeros((X_train.shape[1], 7552))
  #print(attention_plot.shape)
  inputs = tf.convert_to_tensor(inputs)
  inputs = np.tile(inputs, (128,1,1))
  #print(inputs.shape)
  #print(inputs)
  result = ''

  hidden = encoder.initialize_hidden_state()
  #print('..')
  #print(inputs.shape)
  #print(hidden.shape)
  enc_out, enc_hidden = encoder(inputs, hidden)
  #print(enc_out.shape)
  #print(enc_hidden.shape)
  dec_hidden = enc_hidden
  dec_input = tf.random.uniform((BATCH_SIZE, 1))
  #print(z.shape)
  #dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)

  for t in range(X_train.shape[1]):
    predictions, dec_hidden, attention_weights = decoder(z,
                                                         dec_hidden,
                                                         enc_out)
    print(predictions)
    #print(dec_hidden.shape)
    #print(attention_weights.shape)
    print()
    # storing the attention weights to plot later on
    attention_weights = tf.reshape(attention_weights, (-1, ))
    #print(attention_weights.shape)
    attention_plot[t] = attention_weights.numpy()

    predicted_id = tf.argmax(predictions[0]).numpy()
    #print(predicted_id)
    result += dict[predicted_id] + ' '

    if dict[predicted_id] == '<end>':
      return result, attention_plot

    # the predicted ID is fed back into the model
    dec_input = tf.expand_dims([predicted_id], 0)

  return result, attention_plot

# function for plotting the attention weights
def plot_attention(attention, sentence, predicted_sentence):
  fig = plt.figure(figsize=(10,10))
  ax = fig.add_subplot(1, 1, 1)
  ax.matshow(attention, cmap='viridis')

  fontdict = {'fontsize': 14}

  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)
  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)

  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

  plt.show()

def translate(sentence):
  result, attention_plot = evaluate(sentence)

  print('Input: %s' % (sentence))
  print('Predicted translation: {}'.format(result))

  #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]
  #plot_attention(attention_plot, sentence.split(' '), result.split(' '))

checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

translate(X_train[0])

!mkdir encoder
encoder.save_weights('/encoder/')
#decoder.save_weights('/tmp')

